{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "# text preprocessing\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# plots and metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# feature extraction / vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# save and load a file\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df000 = pd.read_csv('isear.csv')\n",
    "df000['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>joy</td>\n",
       "      <td>During the period of falling in love, each tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>fear</td>\n",
       "      <td>When I was involved in a traffic accident.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>anger</td>\n",
       "      <td>When I was driving home after  several days of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sadness</td>\n",
       "      <td>When I lost the person who meant the most to me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>disgust</td>\n",
       "      <td>The time I knocked a deer down - the sight of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7468</td>\n",
       "      <td>anger</td>\n",
       "      <td>Two years back someone invited me to be the tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7469</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I had taken the responsibility to do something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7470</td>\n",
       "      <td>disgust</td>\n",
       "      <td>I was at home and I heard a loud sound of spit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7471</td>\n",
       "      <td>shame</td>\n",
       "      <td>I did not do the homework that the teacher had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7472</td>\n",
       "      <td>guilt</td>\n",
       "      <td>I had shouted at my younger brother and he was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7473 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                               Text\n",
       "0         joy  During the period of falling in love, each tim...\n",
       "1        fear         When I was involved in a traffic accident.\n",
       "2       anger  When I was driving home after  several days of...\n",
       "3     sadness  When I lost the person who meant the most to me. \n",
       "4     disgust  The time I knocked a deer down - the sight of ...\n",
       "...       ...                                                ...\n",
       "7468    anger  Two years back someone invited me to be the tu...\n",
       "7469  sadness  I had taken the responsibility to do something...\n",
       "7470  disgust  I was at home and I heard a loud sound of spit...\n",
       "7471    shame  I did not do the homework that the teacher had...\n",
       "7472    guilt  I had shouted at my younger brother and he was...\n",
       "\n",
       "[7473 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise',\n",
       "       'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df00 = pd.read_csv('text_emotion.csv')\n",
    "df00['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df00 = df00.drop(['tweet_id','author'], axis = 1)\n",
    "df00['Emotion'] = df00['sentiment']\n",
    "df00['Text'] = df00['content']\n",
    "df00 = df00.drop(['content','sentiment'], axis = 1)\n",
    "df00['Emotion'] = df00['Emotion'].replace(['empty'],['neutral'])\n",
    "df00['Emotion'] = df00['Emotion'].replace(['love'],['joy'])\n",
    "df00['Emotion'] = df00['Emotion'].replace(['fun'],['joy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['happy', 'sad', 'surprise', 'disgust', 'anger', 'fear', 'shame'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0 = pd.read_csv('emotion-stimulus.csv')\n",
    "df0['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>joy</td>\n",
       "      <td>I suppose I am happy being so ` tiny' ; it mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>joy</td>\n",
       "      <td>Lennox has always truly wanted to fight for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>joy</td>\n",
       "      <td>He was a professional musician now , still sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>joy</td>\n",
       "      <td>Holmes is happy having the freedom of the hous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>joy</td>\n",
       "      <td>I had problems with tutors trying to encourage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2409</td>\n",
       "      <td>shame</td>\n",
       "      <td>He gets real humiliated and has to leave .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>shame</td>\n",
       "      <td>They aimed for higher status jobs and felt hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2411</td>\n",
       "      <td>shame</td>\n",
       "      <td>He cursed his lack of self-control ; he knew t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2412</td>\n",
       "      <td>shame</td>\n",
       "      <td>Sometimes I've thought I 'll never forget wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2413</td>\n",
       "      <td>shame</td>\n",
       "      <td>GRAHAM TAYLOR will defy the fury of a humiliat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2414 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Emotion                                               Text\n",
       "0        joy  I suppose I am happy being so ` tiny' ; it mea...\n",
       "1        joy  Lennox has always truly wanted to fight for th...\n",
       "2        joy  He was a professional musician now , still sen...\n",
       "3        joy  Holmes is happy having the freedom of the hous...\n",
       "4        joy  I had problems with tutors trying to encourage...\n",
       "...      ...                                                ...\n",
       "2409   shame        He gets real humiliated and has to leave . \n",
       "2410   shame  They aimed for higher status jobs and felt hum...\n",
       "2411   shame  He cursed his lack of self-control ; he knew t...\n",
       "2412   shame   Sometimes I've thought I 'll never forget wha...\n",
       "2413   shame  GRAHAM TAYLOR will defy the fury of a humiliat...\n",
       "\n",
       "[2414 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0['Emotion'] = df0['Emotion'].replace(['happy'],['joy'])\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I experienced this emotion when my grandfather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>when I first moved in , I walked everywhere ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>anger</td>\n",
       "      <td>` Oh ! \" she bleated , her voice high and rath...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fear</td>\n",
       "      <td>However , does the right hon. Gentleman recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sadness</td>\n",
       "      <td>My boyfriend didn't turn up after promising th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  sadness  I experienced this emotion when my grandfather...\n",
       "1  neutral   when I first moved in , I walked everywhere ....\n",
       "2    anger  ` Oh ! \" she bleated , her voice high and rath...\n",
       "3     fear  However , does the right hon. Gentleman recogn...\n",
       "4  sadness  My boyfriend didn't turn up after promising th..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('data_test.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sadness', 'neutral', 'anger', 'fear', 'joy'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(data):    \n",
    "\n",
    "    #remove html markup\n",
    "    data = re.sub(\"(<.*?>)\", \"\", data)\n",
    "\n",
    "    #remove urls\n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    \n",
    "    #remove hashtags and @names\n",
    "    data= re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
    "    data= re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
    "\n",
    "    #remove punctuation and non-ascii digits\n",
    "    data = re.sub(\"(\\\\W|\\\\d)\", \" \", data)\n",
    "    \n",
    "    #remove whitespace\n",
    "    data = data.strip()\n",
    "    \n",
    "    # tokenization with nltk\n",
    "    data = word_tokenize(data)\n",
    "    \n",
    "    # stemming with nltk\n",
    "    #porter = PorterStemmer()\n",
    "    #stem_data = [porter.stem(word) for word in data]\n",
    "        \n",
    "    WNlemma = nltk.WordNetLemmatizer()\n",
    "    stem_data = [WNlemma.lemmatize(t) for t in data]    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(text):\n",
    "    text2 = ' '.join(text)\n",
    "    return text2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>There are tons of other paintings that I thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Yet the dog had grown old and less capable , a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fear</td>\n",
       "      <td>When I get into the tube or the train without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fear</td>\n",
       "      <td>This last may be a source of considerable disq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>anger</td>\n",
       "      <td>She disliked the intimacy he showed towards so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                               Text\n",
       "0  neutral   There are tons of other paintings that I thin...\n",
       "1  sadness  Yet the dog had grown old and less capable , a...\n",
       "2     fear  When I get into the tube or the train without ...\n",
       "3     fear  This last may be a source of considerable disq...\n",
       "4    anger  She disliked the intimacy he showed towards so..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('data_train.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sad', 'neutral', 'anger', 'fear', 'joy', 'surprise', 'disgust',\n",
       "       'shame', 'enthusiasm', 'worry', 'hate', 'boredom', 'guilt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df1.append(df2).append(df0).append(df00).append(df000)\n",
    "df['Emotion'] = df['Emotion'].replace(['sadness','happiness'],['sad','joy'])\n",
    "df = df[df['Emotion'] != 'relief']\n",
    "df['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(convert_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sad</td>\n",
       "      <td>i experienced this emotion when my grandfather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>when i first moved in i walked everywhere but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>anger</td>\n",
       "      <td>oh she bleated her voice high and rather indig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fear</td>\n",
       "      <td>however does the right hon gentleman recognise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sad</td>\n",
       "      <td>my boyfriend didn t turn up after promising th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7468</td>\n",
       "      <td>anger</td>\n",
       "      <td>two years back someone invited me to be the tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7469</td>\n",
       "      <td>sad</td>\n",
       "      <td>i had taken the responsibility to do something...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7470</td>\n",
       "      <td>disgust</td>\n",
       "      <td>i was at home and i heard a loud sound of spit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7471</td>\n",
       "      <td>shame</td>\n",
       "      <td>i did not do the homework that the teacher had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7472</td>\n",
       "      <td>guilt</td>\n",
       "      <td>i had shouted at my younger brother and he was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59688 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                               Text\n",
       "0         sad  i experienced this emotion when my grandfather...\n",
       "1     neutral  when i first moved in i walked everywhere but ...\n",
       "2       anger  oh she bleated her voice high and rather indig...\n",
       "3        fear  however does the right hon gentleman recognise...\n",
       "4         sad  my boyfriend didn t turn up after promising th...\n",
       "...       ...                                                ...\n",
       "7468    anger  two years back someone invited me to be the tu...\n",
       "7469      sad  i had taken the responsibility to do something...\n",
       "7470  disgust  i was at home and i heard a loud sound of spit...\n",
       "7471    shame  i did not do the homework that the teacher had...\n",
       "7472    guilt  i had shouted at my younger brother and he was...\n",
       "\n",
       "[59688 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'] = df['Text'].apply(preprocess_and_tokenize)\n",
    "df['Text'] = df['Text'].apply(join)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T05_11_55.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = f1['text'].apply(convert_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Curve flattening? Kenya records 48 new virus c...\n",
       "1        Victoria and Melbourne Covid trend map: where ...\n",
       "2        NSW and Sydney Covid trend map: where coronavi...\n",
       "3        IT’S BAKE OFF DAY! raising_handsmedium-light_s...\n",
       "4        @DanielAndrewsMP The Liberal party bots are ou...\n",
       "                               ...                        \n",
       "17593    Kanyakumari District Covid19 Updates:\\n\\nCoron...\n",
       "17594    Coimbatore District Covid19 Updates:\\n\\nCorona...\n",
       "17595    @drharshvardhan Ji\\n\\nThink of these Corona wa...\n",
       "17596    Health Minister Harsh Vardhan says, corona vac...\n",
       "17597    @Ravi_freebird @WildCat_Mahi @loverlady12345 @...\n",
       "Name: text, Length: 17598, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.apply(preprocess_and_tokenize)\n",
    "test = test.apply(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sad', 'neutral', 'anger', 'fear', 'joy', 'surprise', 'disgust',\n",
       "       'shame', 'enthusiasm', 'worry', 'hate', 'boredom', 'guilt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lables'] = df['Emotion'].replace(df['Emotion'].unique(), [0,1,2,3,4,5,6,7,8,9,10,11,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Text</th>\n",
       "      <th>Lables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sad</td>\n",
       "      <td>i experienced this emotion when my grandfather...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>when i first moved in i walked everywhere but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>anger</td>\n",
       "      <td>oh she bleated her voice high and rather indig...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fear</td>\n",
       "      <td>however does the right hon gentleman recognise...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sad</td>\n",
       "      <td>my boyfriend didn t turn up after promising th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7468</td>\n",
       "      <td>anger</td>\n",
       "      <td>two years back someone invited me to be the tu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7469</td>\n",
       "      <td>sad</td>\n",
       "      <td>i had taken the responsibility to do something...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7470</td>\n",
       "      <td>disgust</td>\n",
       "      <td>i was at home and i heard a loud sound of spit...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7471</td>\n",
       "      <td>shame</td>\n",
       "      <td>i did not do the homework that the teacher had...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7472</td>\n",
       "      <td>guilt</td>\n",
       "      <td>i had shouted at my younger brother and he was...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59688 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Emotion                                               Text  Lables\n",
       "0         sad  i experienced this emotion when my grandfather...       0\n",
       "1     neutral  when i first moved in i walked everywhere but ...       1\n",
       "2       anger  oh she bleated her voice high and rather indig...       2\n",
       "3        fear  however does the right hon gentleman recognise...       3\n",
       "4         sad  my boyfriend didn t turn up after promising th...       0\n",
       "...       ...                                                ...     ...\n",
       "7468    anger  two years back someone invited me to be the tu...       2\n",
       "7469      sad  i had taken the responsibility to do something...       0\n",
       "7470  disgust  i was at home and i heard a loud sound of spit...       6\n",
       "7471    shame  i did not do the homework that the teacher had...       7\n",
       "7472    guilt  i had shouted at my younger brother and he was...      12\n",
       "\n",
       "[59688 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(reduce_lengthening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.apply(reduce_lengthening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    i experienced this emotion when my grandfather...\n",
       "1    when i first moved in i walked everywhere but ...\n",
       "2    oh she bleated her voice high and rather indig...\n",
       "3    however does the right hon gentleman recognise...\n",
       "4    my boyfriend didn t turn up after promising th...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df['Text']\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        1\n",
       "2        2\n",
       "3        3\n",
       "4        0\n",
       "        ..\n",
       "7468     2\n",
       "7469     0\n",
       "7470     6\n",
       "7471     7\n",
       "7472    12\n",
       "Name: Lables, Length: 59688, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = df['Lables']\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        i experienced this emotion when my grandfather...\n",
       "1        when i first moved in i walked everywhere but ...\n",
       "2        oh she bleated her voice high and rather indig...\n",
       "3        however does the right hon gentleman recognise...\n",
       "4        my boyfriend didn t turn up after promising th...\n",
       "                               ...                        \n",
       "59683    two years back someone invited me to be the tu...\n",
       "59684    i had taken the responsibility to do something...\n",
       "59685    i was at home and i heard a loud sound of spit...\n",
       "59686    i did not do the homework that the teacher had...\n",
       "59687    i had shouted at my younger brother and he was...\n",
       "Name: Text, Length: 59688, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reset_index(drop = True)\n",
    "#X_train = X_train['Text']\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "clc = ClusterCentroids()\n",
    "X_train2,y_train2 = clc.fit_resample(X = X_train_vectorized[:-1000],y = y_train[:-1000])\n",
    "X_val = X_train_vectorized[-1000:]\n",
    "y_val = y_train[-1000:]\n",
    "#clc.fit_resample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer().fit(X_train[:-1000])\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, max_iter=1000, solver='liblinear')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver = 'liblinear',max_iter = 1000, C = 100)\n",
    "model.fit(X_train2, y_train2)\n",
    "\n",
    "#predictions = model.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "clc = CondensedNearestNeighbour()\n",
    "X_train3,y_train3 = clc.fit_resample(X = X_train_vectorized[:-1000],y = y_train[:-1000])\n",
    "X_val2 = X_train_vectorized[-1000:]\n",
    "y_val2 = y_train[-1000:]\n",
    "#clc.fit_resample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=40, max_iter=1000)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model2 = LogisticRegression(max_iter = 1000, C = 40)\n",
    "model2.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861881139966929"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(X_train3,y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(X_val2,y_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train2, y_train2)\n",
    "\n",
    "#ynb_pred = nb.predict(X_test_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6102277610657499"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.365"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_vectorized[:-1000], y_train[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_train_vectorized[-1000:], y_train[-1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(tol=1e-05)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = LinearSVC(tol=1e-05, max_iter = 1000)\n",
    "svc.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8899871078642029"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_train2,y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.385"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Curve flattening Kenya records new virus cases...\n",
       "1        Victoria and Melbourne Covid trend map where c...\n",
       "2        NSW and Sydney Covid trend map where coronavir...\n",
       "3        IT S BAKE OFF DAY raising_handsmedium light_sk...\n",
       "4        The Liberal party bots are out in force even t...\n",
       "                               ...                        \n",
       "17593    Kanyakumari District Covid Updates Corona Spre...\n",
       "17594    Coimbatore District Covid Updates Corona Sprea...\n",
       "17595    Ji Think of these Corona warriors who died in ...\n",
       "17596    Health Minister Harsh Vardhan says corona vacc...\n",
       "17597    COVID Pandemic India ve cases increases with s...\n",
       "Name: text, Length: 17598, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T05_11_55.txt')\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T05_31_23.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T06_01_22.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T06_31_22.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T07_01_23.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T07_31_21.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T08_01_22.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T08_31_22.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T09_01_23.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T09_31_23.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T10_01_23.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T10_31_25.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T11_01_25.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T11_31_25.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T12_01_27.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T12_31_25.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T13_01_28.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T13_32_14.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T14_01_58.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T14_31_58.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T15_31_24.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T16_01_24.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T16_31_22.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T17_01_22.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T17_31_21.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T18_01_20.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T18_31_21.txt'))\n",
    "f1 = f1.append(pd.read_json('Aithon level -3/aithon_level_3_2020-09-22T19_01_22.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Curve flattening? Kenya records 48 new virus c...</td>\n",
       "      <td>IN</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:08:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Victoria and Melbourne Covid trend map: where ...</td>\n",
       "      <td>Erbil, Iraq</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:08:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NSW and Sydney Covid trend map: where coronavi...</td>\n",
       "      <td>Melbourne, Australia</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:08:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>IT’S BAKE OFF DAY! 🙌🏼\\n\\nWho else will be tuni...</td>\n",
       "      <td>Melbourne, Australia</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:06:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@DanielAndrewsMP The Liberal party bots are ou...</td>\n",
       "      <td>Fareham</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:05:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496443</td>\n",
       "      <td>@atomicallyblond @AEHALL1983 @piersmorgan If c...</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:31:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496444</td>\n",
       "      <td>Here are few positive as well as negative impa...</td>\n",
       "      <td></td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:31:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496445</td>\n",
       "      <td>@Terminator_BS_ Corona virus is a strand of vi...</td>\n",
       "      <td>Global</td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:31:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496446</td>\n",
       "      <td>Union Minister Dr. Jitendra Singh reviews COVI...</td>\n",
       "      <td>Grove City, OH</td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:30:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496447</td>\n",
       "      <td>Feathers\\nShop: https://t.co/Vpe1y592fX\\n\\n#fe...</td>\n",
       "      <td>Srinagar, Kashmir</td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:30:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496448 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       Curve flattening? Kenya records 48 new virus c...   \n",
       "1       Victoria and Melbourne Covid trend map: where ...   \n",
       "2       NSW and Sydney Covid trend map: where coronavi...   \n",
       "3       IT’S BAKE OFF DAY! 🙌🏼\\n\\nWho else will be tuni...   \n",
       "4       @DanielAndrewsMP The Liberal party bots are ou...   \n",
       "...                                                   ...   \n",
       "496443  @atomicallyblond @AEHALL1983 @piersmorgan If c...   \n",
       "496444  Here are few positive as well as negative impa...   \n",
       "496445  @Terminator_BS_ Corona virus is a strand of vi...   \n",
       "496446  Union Minister Dr. Jitendra Singh reviews COVI...   \n",
       "496447  Feathers\\nShop: https://t.co/Vpe1y592fX\\n\\n#fe...   \n",
       "\n",
       "                    location    date      time  \n",
       "0                         IN  Sep 22  05:08:45  \n",
       "1                Erbil, Iraq  Sep 22  05:08:34  \n",
       "2       Melbourne, Australia  Sep 22  05:08:33  \n",
       "3       Melbourne, Australia  Sep 22  05:06:02  \n",
       "4                    Fareham  Sep 22  05:05:34  \n",
       "...                      ...     ...       ...  \n",
       "496443                Canada  Sep 19  16:31:50  \n",
       "496444                        Sep 19  16:31:02  \n",
       "496445                Global  Sep 19  16:31:00  \n",
       "496446        Grove City, OH  Sep 19  16:30:46  \n",
       "496447     Srinagar, Kashmir  Sep 19  16:30:40  \n",
       "\n",
       "[496448 rows x 4 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1.reset_index(drop = True)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = f1['text'].apply(convert_emojis)\n",
    "test = test.apply(preprocess_and_tokenize)\n",
    "test = test.apply(join)\n",
    "test = test.apply(reduce_lengthening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc.predict(vect.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1['Emotion'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1['Emotion'] = f1['Emotion'].replace([0,1,2,3,4,5,6,7,8,9,10,11,12], df['Emotion'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1.to_csv('Classified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             joy\n",
       "1             joy\n",
       "2            fear\n",
       "3             joy\n",
       "4           anger\n",
       "           ...   \n",
       "496443        joy\n",
       "496444      worry\n",
       "496445      worry\n",
       "496446        joy\n",
       "496447    neutral\n",
       "Name: Emotion, Length: 496448, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-4a5f30760384>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#import matplotlib.pyplot as plt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#all_words = ' '.join([tws for tws in f1['text']])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_font_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m119\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bilinear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \"\"\"\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \"\"\"\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m             \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munigrams_and_bigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_plurals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocation_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m             \u001b[1;31m# remove stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wordcloud\\tokenization.py\u001b[0m in \u001b[0;36munigrams_and_bigrams\u001b[1;34m(words, stopwords, normalize_plurals, collocation_threshold)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# \"thank much\" from \"thank you very much\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# We don't allow any of the words in the bigram to be stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mbigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0munigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mn_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munigrams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from wordcloud import WordCloud \n",
    "#import matplotlib.pyplot as plt\n",
    "#all_words = ' '.join([tws for tws in f1['text']])\n",
    "wordcloud = WordCloud(width = 1000, height = 600, random_state = 21, max_font_size = 119).generate(all_words)\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig('wordcloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = f1['date'].unique()\n",
    "len(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Curve flattening? Kenya records 48 new virus c...</td>\n",
       "      <td>IN</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:08:45</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Victoria and Melbourne Covid trend map: where ...</td>\n",
       "      <td>Erbil, Iraq</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:08:34</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NSW and Sydney Covid trend map: where coronavi...</td>\n",
       "      <td>Melbourne, Australia</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:08:33</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>IT’S BAKE OFF DAY! 🙌🏼\\n\\nWho else will be tuni...</td>\n",
       "      <td>Melbourne, Australia</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:06:02</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>@DanielAndrewsMP The Liberal party bots are ou...</td>\n",
       "      <td>Fareham</td>\n",
       "      <td>Sep 22</td>\n",
       "      <td>05:05:34</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17601</td>\n",
       "      <td>@atomicallyblond @AEHALL1983 @piersmorgan If c...</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:31:50</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17602</td>\n",
       "      <td>Here are few positive as well as negative impa...</td>\n",
       "      <td></td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:31:02</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17603</td>\n",
       "      <td>@Terminator_BS_ Corona virus is a strand of vi...</td>\n",
       "      <td>Global</td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:31:00</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17604</td>\n",
       "      <td>Union Minister Dr. Jitendra Singh reviews COVI...</td>\n",
       "      <td>Grove City, OH</td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:30:46</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17605</td>\n",
       "      <td>Feathers\\nShop: https://t.co/Vpe1y592fX\\n\\n#fe...</td>\n",
       "      <td>Srinagar, Kashmir</td>\n",
       "      <td>Sep 19</td>\n",
       "      <td>16:30:40</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496448 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      Curve flattening? Kenya records 48 new virus c...   \n",
       "1      Victoria and Melbourne Covid trend map: where ...   \n",
       "2      NSW and Sydney Covid trend map: where coronavi...   \n",
       "3      IT’S BAKE OFF DAY! 🙌🏼\\n\\nWho else will be tuni...   \n",
       "4      @DanielAndrewsMP The Liberal party bots are ou...   \n",
       "...                                                  ...   \n",
       "17601  @atomicallyblond @AEHALL1983 @piersmorgan If c...   \n",
       "17602  Here are few positive as well as negative impa...   \n",
       "17603  @Terminator_BS_ Corona virus is a strand of vi...   \n",
       "17604  Union Minister Dr. Jitendra Singh reviews COVI...   \n",
       "17605  Feathers\\nShop: https://t.co/Vpe1y592fX\\n\\n#fe...   \n",
       "\n",
       "                   location    date      time  Emotion  \n",
       "0                        IN  Sep 22  05:08:45      joy  \n",
       "1               Erbil, Iraq  Sep 22  05:08:34      joy  \n",
       "2      Melbourne, Australia  Sep 22  05:08:33     fear  \n",
       "3      Melbourne, Australia  Sep 22  05:06:02      joy  \n",
       "4                   Fareham  Sep 22  05:05:34    anger  \n",
       "...                     ...     ...       ...      ...  \n",
       "17601                Canada  Sep 19  16:31:50      joy  \n",
       "17602                        Sep 19  16:31:02    worry  \n",
       "17603                Global  Sep 19  16:31:00    worry  \n",
       "17604        Grove City, OH  Sep 19  16:30:46      joy  \n",
       "17605     Srinagar, Kashmir  Sep 19  16:30:40  neutral  \n",
       "\n",
       "[496448 rows x 5 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = f1[f1['date'] == date[0]]\n",
    "f2.to_csv('1.csv')\n",
    "f3 = f1[f1['date'] == date[1]]\n",
    "f3.to_csv('2.csv')\n",
    "f4 = f1[f1['date'] == date[2]]\n",
    "f4.to_csv('3.csv')\n",
    "f5 = f1[f1['date'] == date[3]]\n",
    "f5.to_csv('4.csv')\n",
    "f6 = f1[f1['date'] == date[4]]\n",
    "f6.to_csv('5.csv')\n",
    "f7 = f1[f1['date'] == date[5]]\n",
    "f7.to_csv('6.csv')\n",
    "f8 = f1[f1['date'] == date[6]]\n",
    "f8.to_csv('7.csv')\n",
    "f9 = f1[f1['date'] == date[7]]\n",
    "f9.to_csv('8.csv')\n",
    "f10 = f1[f1['date'] == date[8]]\n",
    "f10.to_csv('9.csv')\n",
    "f11 = f1[f1['date'] == date[9]]\n",
    "f11.to_csv('10.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
